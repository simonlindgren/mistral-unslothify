{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75df6a32-d294-4fdb-b775-0ef12e660e49",
   "metadata": {},
   "source": [
    "Install needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61c627-f385-44b4-9646-0d70025477cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers -q\n",
    "%pip install --upgrade torch accelerate -q\n",
    "%pip install bitsandbytes -q\n",
    "%pip install auto-gptq -q\n",
    "%pip install unsloth -q\n",
    "%pip install unsloth-zoo -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df81604-5bdf-4bb1-b37d-8a5a35171bc2",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71af0df-df8f-4d92-b708-47213f4a3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8211f39-faee-4490-b44d-9762256dccf3",
   "metadata": {},
   "source": [
    "Load a model from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30ab65-1ea5-4f5d-a870-a770bab83bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "max_seq_length = 2048  # Adjust as necessary\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a6e17-acd4-4abe-a2de-c9a19576f8fd",
   "metadata": {},
   "source": [
    "Test prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac89d97-af2a-4b96-8152-5b73b70db85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "prompt = \"In what way has AI changed society?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=120)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response[len(prompt):].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f94cd-74e0-4cea-8052-67c63f0152df",
   "metadata": {},
   "source": [
    "Prepare for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7460912-b692-4d0d-a91f-7528e20c050b",
   "metadata": {},
   "source": [
    "Add [LoRA adapters](https://arxiv.org/abs/2106.09685)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb9e23-c708-413c-9073-7ac2fd9a433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    random_state = 1337,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372f991-8efd-4314-b4bf-2aa39ac4575d",
   "metadata": {},
   "source": [
    "Prepare domain data. The model will not 'memorize' these text items as 'facts', but the model will update its weights so that it can better generate responses that are aligned with the specific language patterns, terminology, and nuances of the domain. A fine-tuned LLM would improve not only in generating text in the fine-tuned style but also in recognizing and discerning nuances of that style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e352f-49cf-40ce-9fc6-0da4226fff0f",
   "metadata": {
    "id": "6e917277-8992-4b00-9a06-ed57835aa42b"
   },
   "outputs": [],
   "source": [
    "jsonl_file = \"domain.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498a187-4fa7-4820-9af3-7e8cb9af4019",
   "metadata": {
    "id": "gg9jVHfsVAze"
   },
   "source": [
    "Load and reformat the domain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4faff6-9f60-4cdb-b893-b10fde5b201e",
   "metadata": {
    "id": "fDKhMvkiUTqM"
   },
   "outputs": [],
   "source": [
    "# use the dataset loader by Huggingface and some formatting functions\n",
    "dataset = load_dataset(\"json\", data_files=jsonl_file, split=\"train\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def format_text(examples):\n",
    "    texts = [note + tokenizer.pad_token for note in examples[\"text\"]]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(format_text, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9edce-3da1-4827-b90c-3cfa81f3192e",
   "metadata": {
    "id": "0ScQDOp4VaVD"
   },
   "source": [
    "Tokenize the domain data with the pre-trained model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b5bb6-c6d7-4215-b03e-d19150f453b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "af854e58d42a431fa48535e8e8aa6644",
      "00ca831faf204bcf91fc78a5724cd8e6",
      "9c1aa1daf38e4aeb8d9858929be7ae14",
      "f30bcfdebbcc4391bb45b283ce0b48ce",
      "fff442eb8b5e4dbc9ddcf2f47b5bf414",
      "e51107af09b94c2d85b92ab8109d6af4",
      "1f9191103f414d559285cfed5927f528",
      "92a9b5b25a284609a138d4a9a209b08e",
      "2fb703591d6543ccaff0f98ace7f1e93",
      "a0074981ae6845208ae79c723866466a",
      "2c441204bcc94f10a9bdfc72db3d659d"
     ]
    },
    "id": "6lIjxLTbVTbp",
    "outputId": "77b865f1-0fe8-411b-d569-e9580bdb59a3"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_texts(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_texts, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f5d04-764f-4c2b-9236-868092c091fd",
   "metadata": {
    "id": "OlT6-2JUYWPT"
   },
   "source": [
    "## 4. Fine-tune the model\n",
    "\n",
    "We want the *training loss* to decrease. A loss value around 2-3 is reasonable, if it gets close to 1.0 or drops below, the predictions will be highly confident, but also with some risk of overfitting, meaning that the model has learned the training data too well and may not perform as effectively on unseen data.\n",
    "\n",
    "*See `README.md` for details about which parameters to tweak to avoid overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e0279-5874-4787-b231-b7e31089d102",
   "metadata": {
    "id": "cWJbz5OqV9AE"
   },
   "outputs": [],
   "source": [
    "# Remove unneeded columns and set format for PyTorch\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])  # Keep only tokenized columns\n",
    "tokenized_dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd58839-cd95-4634-9eff-c78d6756d928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "cc5a9fa9-18c3-49ec-b0ce-5f45d38bef5a",
    "outputId": "eff0558d-4d9d-435a-aced-02d90ad43de4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    max_seq_length=1024,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=0.00001, # <<<<<<< THE HIGHER THE RATE THE FASTER TO OVERFIT\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.3,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=1337,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6200295-7174-4571-b3f2-2e4e9832b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicity save the model and tokenizer after training\n",
    "trainer.save_model(\"outputs\")  # Saves the model, tokenizer, and training args\n",
    "tokenizer.save_pretrained(\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24f544-fa0d-486b-b5d5-3bcb1eb10245",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Specify the path to your fine-tuned model\n",
    "fine_tuned_model_path = \"outputs\"  # Path where your fine-tuned model is saved\n",
    "\n",
    "# Load the fine-tuned model and tokenizer using unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_path,\n",
    "    max_seq_length=2048  # Adjust to match your desired sequence length\n",
    ")\n",
    "\n",
    "# Prepare the model for inference\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa0c58-5994-4d8e-a36f-d39a12a3220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Is climate change real?\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
